{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf # http://blog.topspeedsnail.com/archives/10399\n",
    "from sklearn.preprocessing import scale # 使用scikit-learn进行数据预处理\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('./UJIndoorLoc/trainingData.csv', header = 0)\n",
    "#print(training_data.head())\n",
    "train_x = scale(np.asarray(training_data.ix[:, 0:520]))\n",
    "train_y = np.asarray(training_data['BUILDINGID'].map(str) + training_data['FLOOR'].map(str))\n",
    "train_y = np.asarray(pd.get_dummies(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_dataset = pd.read_csv('./UJIndoorLoc/validationData.csv', header = 0)\n",
    "test_x = scale(np.asarray(test_dataset.ix[:, 0:520]))\n",
    "test_y = np.asarray(test_dataset['BUILDINGID'].map(str) + test_dataset['FLOOR'].map(str))\n",
    "test_y = np.asarray(pd.get_dummies(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = train_y.shape[1]\n",
    "X = tf.placeholder(tf.float32, shape=[None, 520])\n",
    "Y = tf.placeholder(tf.float32, [None, output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义神经网络\n",
    "def neural_networks():\n",
    "    # --------------------- Encoder -------------------- #\n",
    "    e_w_1 = tf.Variable(tf.truncated_normal([520, 256], stddev = 0.1))\n",
    "    e_b_1 = tf.Variable(tf.constant(0.0, shape=[256]))\n",
    "    e_w_2 = tf.Variable(tf.truncated_normal([256, 128], stddev = 0.1))\n",
    "    e_b_2 = tf.Variable(tf.constant(0.0, shape=[128]))\n",
    "    e_w_3 = tf.Variable(tf.truncated_normal([128, 64], stddev = 0.1))\n",
    "    e_b_3 = tf.Variable(tf.constant(0.0, shape=[64]))\n",
    "    # --------------------- Decoder  ------------------- #\n",
    "    d_w_1 = tf.Variable(tf.truncated_normal([64, 128], stddev = 0.1))\n",
    "    d_b_1 = tf.Variable(tf.constant(0.0, shape=[128]))\n",
    "    d_w_2 = tf.Variable(tf.truncated_normal([128, 256], stddev = 0.1))\n",
    "    d_b_2 = tf.Variable(tf.constant(0.0, shape=[256]))\n",
    "    d_w_3 = tf.Variable(tf.truncated_normal([256, 520], stddev = 0.1))\n",
    "    d_b_3 = tf.Variable(tf.constant(0.0, shape=[520]))\n",
    "    # --------------------- DNN  ------------------- #\n",
    "    w_1 = tf.Variable(tf.truncated_normal([64, 128], stddev = 0.1))\n",
    "    b_1 = tf.Variable(tf.constant(0.0, shape=[128]))\n",
    "    w_2 = tf.Variable(tf.truncated_normal([128, 128], stddev = 0.1))\n",
    "    b_2 = tf.Variable(tf.constant(0.0, shape=[128]))\n",
    "    w_3 = tf.Variable(tf.truncated_normal([128, output], stddev = 0.1))\n",
    "    b_3 = tf.Variable(tf.constant(0.0, shape=[output]))\n",
    "    #########################################################\n",
    "    layer_1 = tf.nn.tanh(tf.add(tf.matmul(X,       e_w_1), e_b_1))\n",
    "    layer_2 = tf.nn.tanh(tf.add(tf.matmul(layer_1, e_w_2), e_b_2))\n",
    "    encoded = tf.nn.tanh(tf.add(tf.matmul(layer_2, e_w_3), e_b_3))\n",
    "    layer_4 = tf.nn.tanh(tf.add(tf.matmul(encoded, d_w_1), d_b_1))\n",
    "    layer_5 = tf.nn.tanh(tf.add(tf.matmul(layer_4, d_w_2), d_b_2))\n",
    "    decoded = tf.nn.tanh(tf.add(tf.matmul(layer_5, d_w_3), d_b_3))\n",
    "    layer_7 = tf.nn.tanh(tf.add(tf.matmul(encoded, w_1),   b_1))\n",
    "    layer_8 = tf.nn.tanh(tf.add(tf.matmul(layer_7, w_2),   b_2))\n",
    "    out = tf.nn.softmax(tf.add(tf.matmul( layer_8, w_3),   b_3))\n",
    "    return (decoded, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 训练神经网络\n",
    "def train_neural_networks():\n",
    "    decoded, predict_output = neural_networks()\n",
    "    \n",
    "    us_cost_function = tf.reduce_mean(tf.pow(X - decoded, 2))\n",
    "    s_cost_function = -tf.reduce_sum(Y * tf.log(predict_output))\n",
    "    us_optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(us_cost_function)\n",
    "    s_optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(s_cost_function)\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(predict_output, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    training_epochs = 20\n",
    "    batch_size = 10\n",
    "    total_batches = training_data.shape[0]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # ------------ Training Autoencoders - Unsupervised Learning ----------- #\n",
    "        # autoencoder是一种非监督学习算法，他利用反向传播算法，让目标值等于输入值\n",
    "        for epoch in range(training_epochs):\n",
    "            epoch_costs = np.empty(0)\n",
    "            for b in range(total_batches):\n",
    "                offset = (b * batch_size) % (train_x.shape[0] - batch_size)\n",
    "                batch_x = train_x[offset:(offset + batch_size), :]\n",
    "                _, c = sess.run([us_optimizer, us_cost_function], feed_dict={X: batch_x})\n",
    "                epoch_costs = np.append(epoch_costs, c)\n",
    "            print('Epoch: ', epoch, ' Loss: ', np.mean(epoch_costs))\n",
    "        print(\"------------------------------------------------------------------\")\n",
    "        \n",
    "        # ---------------- Training NN - Supervised Learning ------------------ #\n",
    "        for epoch in range(training_epochs):\n",
    "            epoch_costs = np.empty(0)\n",
    "            for b in range(total_batches):\n",
    "                offset = (b * batch_size) % (train_x.shape[0] - batch_size)\n",
    "                batch_x = train_x[offset:(offset + batch_size), :]\n",
    "                batch_y = train_y[offset:(offset + batch_size), :]\n",
    "                _, c = sess.run([s_optimizer, s_cost_function], feed_dict={X:batch_x, Y:batch_y})\n",
    "                epoch_costs = np.append(epoch_costs, c)\n",
    "            \n",
    "            accuracy_in_train_set = sess.run(accuracy, feed_dict={X:train_x, Y:train_y})\n",
    "            accuracy_in_test_set = sess.run(accuracy, feed_dict={X:test_x, Y:test_y})\n",
    "            print('Epoch: ', epoch, ' Loss: ', np.mean(epoch_costs), ' Accuracy: ', accuracy_in_train_set, ' ', accuracy_in_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Loss:  0.811163063365\n",
      "Epoch:  1  Loss:  0.745404697547\n",
      "Epoch:  2  Loss:  0.730484397802\n",
      "Epoch:  3  Loss:  0.722523403331\n",
      "Epoch:  4  Loss:  0.717252474601\n",
      "Epoch:  5  Loss:  0.713395552334\n",
      "Epoch:  6  Loss:  0.710352347159\n",
      "Epoch:  7  Loss:  0.707823287564\n",
      "Epoch:  8  Loss:  0.705657089711\n",
      "Epoch:  9  Loss:  0.703769226292\n",
      "Epoch:  10  Loss:  0.702106146214\n",
      "Epoch:  11  Loss:  0.700628797791\n",
      "Epoch:  12  Loss:  0.699305994264\n",
      "Epoch:  13  Loss:  0.698111438284\n",
      "Epoch:  14  Loss:  0.697022781142\n",
      "Epoch:  15  Loss:  0.696022743388\n",
      "Epoch:  16  Loss:  0.695096936329\n",
      "Epoch:  17  Loss:  0.694233497918\n",
      "Epoch:  18  Loss:  0.693423232996\n",
      "Epoch:  19  Loss:  0.692658683423\n",
      "------------------------------------------------------------------\n",
      "Epoch:  0  Loss:  1.39020893846  Accuracy:  0.755279   0.593159\n",
      "Epoch:  1  Loss:  0.472183999188  Accuracy:  0.911923   0.711971\n",
      "Epoch:  2  Loss:  0.251755594388  Accuracy:  0.959272   0.718272\n",
      "Epoch:  3  Loss:  0.199712017755  Accuracy:  0.917691   0.679568\n",
      "Epoch:  4  Loss:  0.158347905355  Accuracy:  0.98731   0.738974\n",
      "Epoch:  5  Loss:  0.161306657493  Accuracy:  0.991624   0.738074\n",
      "Epoch:  6  Loss:  0.157772277692  Accuracy:  0.99037   0.739874\n",
      "Epoch:  7  Loss:  0.112355393883  Accuracy:  0.994081   0.748875\n",
      "Epoch:  8  Loss:  0.105269542735  Accuracy:  0.995636   0.750675\n",
      "Epoch:  9  Loss:  0.110945194742  Accuracy:  0.992627   0.719172\n",
      "Epoch:  10  Loss:  0.0961873328008  Accuracy:  0.995185   0.752475\n",
      "Epoch:  11  Loss:  0.0779360772832  Accuracy:  0.996389   0.740774\n",
      "Epoch:  12  Loss:  0.0603794037505  Accuracy:  0.994332   0.745275\n",
      "Epoch:  13  Loss:  0.147252067979  Accuracy:  0.990169   0.739874\n",
      "Epoch:  14  Loss:  0.122115488655  Accuracy:  0.994132   0.744374\n",
      "Epoch:  15  Loss:  0.0856258698724  Accuracy:  0.995636   0.733573\n",
      "Epoch:  16  Loss:  0.0783164579705  Accuracy:  0.99679   0.746175\n",
      "Epoch:  17  Loss:  0.0652983305321  Accuracy:  0.99684   0.753375\n",
      "Epoch:  18  Loss:  0.0615127251595  Accuracy:  0.996138   0.743474\n",
      "Epoch:  19  Loss:  0.0863316224366  Accuracy:  0.995887   0.742574\n"
     ]
    }
   ],
   "source": [
    "train_neural_networks()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
